{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config_file import get_config\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4369334640"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def get_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "    if tokenizer_path.exists():\n",
    "        # load\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    else:\n",
    "        # build\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(min_frequency=2, show_progress=True, \n",
    "                                   special_tokens=[\"[UNK]\", \"[SOS]\", \"[EOS]\", \"[PAD]\"])\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm why use token_to_id? Why not use encode? And if so, why not use token_to_id when max_len? ids?\n",
    "# what shape does maked_fill_ in attention expect? (batch, h, seq, seq) is attention matrix. based on that we are making masks to be (1, 1, seq_len)...\n",
    "# shape of causal mask also\n",
    "# cant i use torch.cat with lists? - Nope. Also tensor scalars not allowed - hence, [tokencoded SOS]\n",
    "# whats the result type of encode, token_to_id etc etc - not tensors? can we make it tensors? - Nope\n",
    "# difference between dtype=torch.int64 and .int()\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, ds, config, tokenizer_src, tokenizer_tgt):\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.seq_len = config[\"seq_len\"]\n",
    "        self.lang_src = config[\"lang_src\"]\n",
    "        self.lang_tgt = config[\"lang_tgt\"]\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.sos_token = torch.tensor([self.tokenizer_src.token_to_id('[SOS]')]).int()\n",
    "        self.eos_token = torch.tensor([self.tokenizer_src.token_to_id('[EOS]')]).int()\n",
    "        self.pad_token = torch.tensor([self.tokenizer_src.token_to_id('[PAD]')]).int()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        src_tgt_pair = self.ds[index]['translation']\n",
    "        src_text = src_tgt_pair.get(self.lang_src)\n",
    "        tgt_text = src_tgt_pair.get(self.lang_tgt)\n",
    "\n",
    "        src_ids = self.tokenizer_src.encode(src_text).ids\n",
    "        tgt_ids = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        enc_pad_count = self.seq_len - len(src_ids) - 2\n",
    "        dec_pad_count = self.seq_len - len(tgt_ids) - 1\n",
    "        if enc_pad_count < 0 or dec_pad_count < 0:\n",
    "            raise ValueError(\"Sentence too long\")\n",
    "\n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(src_ids).int(), \n",
    "            torch.tensor([self.pad_token] * enc_pad_count).int()\n",
    "        ])\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token, \n",
    "            torch.tensor(tgt_ids).int(),\n",
    "            torch.tensor([self.pad_token] * dec_pad_count).int()\n",
    "        ])\n",
    "\n",
    "        label = torch.cat([\n",
    "            torch.tensor(tgt_ids).int(),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * dec_pad_count).int()\n",
    "        ])\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), \n",
    "            # encoder mask: (1, 1, seq_len) -> Has 1 when there is text and 0 when there is pad (no text)\n",
    "            \n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            # (1, seq_len) and (1, seq_len, seq_len)\n",
    "            # Will get 0 for all pads. And 0 for earlier text.\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text\n",
    "            }\n",
    "    \n",
    "def causal_mask(size):\n",
    "    return torch.tril(torch.ones(1, size, size)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not use batch encoding to speed things up? > NEED TO SORT THIS OUT\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = datasets.load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
    "    tokenizer_src = get_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])        \n",
    "    # max_length_src = 0\n",
    "    # max_length_tgt = 0\n",
    "    # for item in ds_raw['translation']:\n",
    "    #     src_ids = tokenizer_src.encode(item[config[\"lang_src\"]]).ids\n",
    "    #     tgt_ids = tokenizer_tgt.encode(item[config[\"lang_tgt\"]]).ids\n",
    "    #     max_length_src = max(max_length_src, len(src_ids))\n",
    "    #     max_length_tgt = max(max_length_tgt, len(tgt_ids))\n",
    "    # print(f'[INFO] Longest sentence in src languagee  - {config[\"lang_src\"]} - contains {max_length_src} words')\n",
    "    # print(f'[INFO] Longest sequence in tgt language - {config[\"lang_tgt\"]} - contains {max_length_tgt} words')\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, lengths=[0.9, 0.1])\n",
    "    train_ds = BilingualDataset(train_ds_raw, config, tokenizer_src, tokenizer_tgt)\n",
    "    val_ds = BilingualDataset(val_ds_raw, config, tokenizer_src, tokenizer_tgt)\n",
    "    train_dl = DataLoader(train_ds, shuffle=True, batch_size=config[\"batch_size\"])\n",
    "    val_dl = DataLoader(val_ds, shuffle=True, batch_size=1)\n",
    "    return train_dl, val_dl, tokenizer_src, tokenizer_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['encoder_input', 'decoder_input', 'encoder_mask', 'decoder_mask', 'label', 'src_text', 'tgt_text'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 349])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values['encoder_input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erav2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
